name: Scheduled Data Pipeline

on:
  # 1. Allows you to run this manually from the "Actions" tab
  workflow_dispatch:
  
  # 2. Runs on a schedule
  schedule:
    # Runs "at minute 0 past every 3rd hour" (e.g., 00:00, 03:00, 06:00 UTC)
    - cron: '0 */3 * * *'

jobs:
  scrape-and-build:
    runs-on: ubuntu-latest
    
    steps:
      - name: Check out repository
        uses: actions/checkout@v4

      - name: Set up Python 3.10
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      
      - name: Crawl4ai Setup
        run: |
          crawl4ai-setup

      - name: Run Scraper
        env:
          # Get the API key from your repo's secrets
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python main.py

      - name: Run Corpus Builder
        run: |
          python build_corpus.py

      - name: Commit and push changes
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # Message for the commit
          commit_message: "Auto: Update product corpus"
          
          # The files to add and commit
          file_pattern: "out/daraz_products_corpus.md out/daraz_products_corpus.txt out/daraz_products_corpus.jsonl result/*/*.json"
          
          # Git user details for the commit
          commit_user_name: GitHub Actions Bot
          commit_user_email: actions@github.com
          commit_author: GitHub Actions Bot <actions@github.com>